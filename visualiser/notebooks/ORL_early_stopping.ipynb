{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b86a615c",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "This notebook tests the effectiveness of early stopping criterion, and re-runs trials to test more thoroughly. IQL early stopping criteria is used instead of the TD3+BC one if 'iql' is in the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa23970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Crago\\AppData\\Local\\Temp\\ipykernel_23028\\1443756178.py:306: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return 1 - (RSS/TSS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "offline_td3_purebc_0_0: \n",
      "\tReward obtained is 283.82/283.82, 100.00%\n",
      "\n",
      "offline_td3_purebc_0_1: \n",
      "\tReward obtained is 87.70/283.09, 30.98%\n",
      "\n",
      "offline_td3_purebc_0_2: \n",
      "\tReward obtained is 280.06/280.61, 99.81%\n",
      "\n",
      "offline_td3_purebc_1_0: \n",
      "\tReward obtained is 110.68/230.34, 48.05%\n",
      "\n",
      "offline_td3_purebc_1_1: \n",
      "\tReward obtained is 221.64/234.10, 94.68%\n",
      "\n",
      "offline_td3_purebc_1_2: \n",
      "\tReward obtained is 103.77/245.18, 42.32%\n",
      "\n",
      "offline_td3_purebc_2_0: \n",
      "\tReward obtained is 96.08/247.57, 38.81%\n",
      "\n",
      "offline_td3_purebc_2_1: \n",
      "\tReward obtained is 261.05/261.05, 100.00%\n",
      "\n",
      "offline_td3_purebc_2_2: \n",
      "\tReward obtained is 46.24/238.83, 19.36%\n",
      "\n",
      "offline_td3_purebc_3_0: \n",
      "\tReward obtained is 242.87/242.87, 100.00%\n",
      "\n",
      "offline_td3_purebc_3_1: \n",
      "\tReward obtained is 191.39/234.61, 81.58%\n",
      "\n",
      "offline_td3_purebc_3_2: \n",
      "\tReward obtained is 37.17/244.92, 15.18%\n",
      "\n",
      "offline_td3_purebc_4_0: \n",
      "\tReward obtained is 200.52/200.52, 100.00%\n",
      "\n",
      "offline_td3_purebc_4_1: \n",
      "\tReward obtained is 237.89/256.77, 92.64%\n",
      "\n",
      "offline_td3_purebc_4_2: \n",
      "\tReward obtained is 256.33/256.33, 100.00%\n",
      "\n",
      "offline_td3_purebc_5_0: \n",
      "\tReward obtained is 156.87/265.89, 59.00%\n",
      "\n",
      "offline_td3_purebc_5_1: \n",
      "\tReward obtained is 245.13/245.13, 100.00%\n",
      "\n",
      "offline_td3_purebc_5_2: \n",
      "\tReward obtained is 262.22/268.48, 97.67%\n",
      "\n",
      "offline_td3_purebc_6_0: \n",
      "\tReward obtained is 241.19/241.19, 100.00%\n",
      "\n",
      "offline_td3_purebc_6_1: \n",
      "\tReward obtained is 233.33/238.27, 97.92%\n",
      "\n",
      "offline_td3_purebc_6_2: \n",
      "\tReward obtained is 241.79/241.79, 100.00%\n",
      "\n",
      "offline_td3_purebc_7_0: \n",
      "\tReward obtained is 242.50/242.50, 100.00%\n",
      "\n",
      "offline_td3_purebc_7_1: \n",
      "\tReward obtained is 238.88/238.88, 100.00%\n",
      "\n",
      "offline_td3_purebc_7_2: \n",
      "\tReward obtained is 237.26/237.26, 100.00%\n",
      "\n",
      "offline_td3_purebc_8_0: \n",
      "\tReward obtained is 127.90/273.27, 46.80%\n",
      "\n",
      "offline_td3_purebc_8_1: \n",
      "\tReward obtained is 215.23/268.82, 80.06%\n",
      "\n",
      "offline_td3_purebc_8_2: \n",
      "\tReward obtained is 154.11/268.74, 57.34%\n",
      "\n",
      "offline_td3_purebc_9_0: \n",
      "\tReward obtained is 50.78/241.56, 21.02%\n",
      "\n",
      "offline_td3_purebc_9_1: \n",
      "\tReward obtained is 158.25/158.25, 100.00%\n",
      "\n",
      "offline_td3_purebc_9_2: \n",
      "\tReward obtained is 32.76/253.44, 12.92%\n",
      "\n",
      "offline_td3_purebc_20_0: \n",
      "\tReward obtained is 168.95/271.41, 62.25%\n",
      "\n",
      "offline_td3_purebc_20_1: \n",
      "\tReward obtained is 277.07/277.07, 100.00%\n",
      "\n",
      "offline_td3_purebc_20_2: \n",
      "\tReward obtained is 146.87/266.47, 55.12%\n",
      "\n",
      "offline_td3_purebc_21_0: \n",
      "\tReward obtained is 269.08/269.08, 100.00%\n",
      "\n",
      "offline_td3_purebc_21_1: \n",
      "\tReward obtained is 238.95/238.95, 100.00%\n",
      "\n",
      "offline_td3_purebc_21_2: \n",
      "\tReward obtained is 210.52/279.80, 75.24%\n",
      "\n",
      "offline_td3_purebc_22_0: \n",
      "\tReward obtained is 267.78/267.78, 100.00%\n",
      "\n",
      "offline_td3_purebc_22_1: \n",
      "\tReward obtained is 175.98/175.98, 100.00%\n",
      "\n",
      "offline_td3_purebc_22_2: \n",
      "\tReward obtained is 254.37/254.37, 100.00%\n",
      "\n",
      "offline_td3_purebc_23_0: \n",
      "\tReward obtained is 217.29/252.84, 85.94%\n",
      "\n",
      "offline_td3_purebc_23_1: \n",
      "\tReward obtained is 149.83/256.02, 58.52%\n",
      "\n",
      "offline_td3_purebc_23_2: \n",
      "\tReward obtained is 50.86/225.99, 22.50%\n",
      "\n",
      "offline_td3_purebc_24_0: \n",
      "\tReward obtained is 276.94/276.94, 100.00%\n",
      "\n",
      "offline_td3_purebc_24_1: \n",
      "\tReward obtained is 277.56/277.56, 100.00%\n",
      "\n",
      "offline_td3_purebc_24_2: \n",
      "\tReward obtained is 252.06/252.06, 100.00%\n",
      "\n",
      "offline_td3_purebc_25_0: \n",
      "\tReward obtained is 199.19/199.19, 100.00%\n",
      "\n",
      "offline_td3_purebc_25_1: \n",
      "\tReward obtained is 142.60/260.24, 54.79%\n",
      "\n",
      "offline_td3_purebc_25_2: \n",
      "\tReward obtained is 140.08/140.08, 100.00%\n",
      "\n",
      "offline_td3_purebc_26_0: \n",
      "\tReward obtained is 114.67/268.24, 42.75%\n",
      "\n",
      "offline_td3_purebc_26_1: \n",
      "\tReward obtained is 155.60/232.02, 67.06%\n",
      "\n",
      "offline_td3_purebc_26_2: \n",
      "\tReward obtained is 130.40/241.25, 54.05%\n",
      "\n",
      "offline_td3_purebc_27_0: \n",
      "\tReward obtained is 83.07/278.52, 29.83%\n",
      "\n",
      "offline_td3_purebc_27_1: \n",
      "\tReward obtained is 276.77/276.77, 100.00%\n",
      "\n",
      "offline_td3_purebc_27_2: \n",
      "\tReward obtained is 259.76/274.33, 94.69%\n",
      "\n",
      "offline_td3_purebc_28_0: \n",
      "\tReward obtained is 268.25/268.25, 100.00%\n",
      "\n",
      "offline_td3_purebc_28_1: \n",
      "\tReward obtained is 266.42/266.42, 100.00%\n",
      "\n",
      "offline_td3_purebc_28_2: \n",
      "\tReward obtained is 272.92/272.92, 100.00%\n",
      "\n",
      "offline_td3_purebc_29_0: \n",
      "\tReward obtained is 270.47/270.47, 100.00%\n",
      "\n",
      "offline_td3_purebc_29_1: \n",
      "\tReward obtained is 271.53/271.53, 100.00%\n",
      "\n",
      "offline_td3_purebc_29_2: \n",
      "\tReward obtained is 272.62/272.62, 100.00%\n",
      "\n",
      "Average returns of 78.98+-28.33%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from decouple import config\n",
    "MAIN_PATH = config('MAIN_PATH')\n",
    "sys.path.insert(1, MAIN_PATH)\n",
    "from visualiser.core import ExperimentVisualise, plot_testing_rewards, plot_training_logs, plot_value_function\n",
    "from visualiser.core import plot_episode, plot_training_action_summary, plot_training_logs\n",
    "from visualiser.core import display_commands, plot_testing_metric, display_commands_v2#, plot_testing_average_metric\n",
    "from metrics.metrics import time_in_range\n",
    "from metrics.statistics import calc_stats\n",
    "from agents.models.actor_critic_td3_bc import ActorCritic as TD3BC_ActorCritic\n",
    "from agents.models.actor_critic_iql import ActorCritic as IQL_ActorCritic\n",
    "import numpy.polynomial.polynomial as poly \n",
    "from utils.worker import OnPolicyWorker, OffPolicyWorker, OfflineSampler\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "import matplotlib.gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "from utils.logger import setup_folders, Logger\n",
    "\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "EXP_NAME = \"offline_iql_t3_6_0\"\n",
    "\n",
    "LABEL = EXP_NAME\n",
    "\n",
    "# custom functions\n",
    "\n",
    "\n",
    "PI_GRAD_THRESHOLD = 140\n",
    "PI_CONCAV_THRESHOLD = 0.00008\n",
    "PI_CONCAV_CONVERGENCE_THRESHOLD = 0.0001\n",
    "PI_CONCAV_CONVERGENCE_N = 3\n",
    "MONOTONIC_EPISLON = 1e-5\n",
    "\n",
    "TITLE_FONT_SIZE = 24\n",
    "AXIS_FONT_SIZE = 16\n",
    "\n",
    "TAKE_BEST = True\n",
    "SHOW_GRAPHS = False\n",
    "\n",
    "NAME_PREFIX = \"offline_td3_purebc\"\n",
    "USE_IQL_CRITERIA = 'iql' in NAME_PREFIX.lower()\n",
    "\n",
    "def limit(n,t,b): return max(min(n,t),b)\n",
    "\n",
    "def check_end_condition_total(\n",
    "        val_grads, val_losses, pi_grads, steps,\n",
    "        debug_show=False\n",
    "    ):\n",
    "    \n",
    "    # default to last network\n",
    "    epochs = len(val_grads)\n",
    "    total_ind = epochs\n",
    "\n",
    "\n",
    "\n",
    "    pi_grad_grads = []\n",
    "    for n in range(epochs-1):\n",
    "        dy = pi_grads[n+1] - pi_grads[n]\n",
    "        dx = steps[n+1] - steps[n]\n",
    "        pi_grad_grads.append( (dy / dx) )\n",
    "\n",
    "    if debug_show: \n",
    "        print(pi_grads)\n",
    "        print(pi_grad_grads)\n",
    "    \n",
    "    rough_monotonic = all([pi_grad_grad < MONOTONIC_EPISLON for pi_grad_grad in pi_grad_grads])\n",
    "    if not rough_monotonic and debug_show: print(\"WARNING! Not monotonic\")\n",
    "    \n",
    "\n",
    "    # calculated based on pi grad threshold\n",
    "    for n, pi_grad in enumerate(pi_grads):\n",
    "        if abs(pi_grad) < PI_GRAD_THRESHOLD:\n",
    "            total_ind = (n - 1)\n",
    "            break\n",
    "\n",
    "    total_ind = min(max(total_ind, 0), epochs -1)\n",
    "\n",
    "    return total_ind\n",
    "\n",
    "def check_end_condition_total_alt(\n",
    "        val_grads, val_losses, pi_grads, steps,\n",
    "        debug_show=False\n",
    "    ):\n",
    "    \n",
    "    # default to last network\n",
    "    epochs = len(val_grads)\n",
    "    total_ind = epochs\n",
    "\n",
    "\n",
    "\n",
    "    pi_grad_grads = []\n",
    "    for n in range(epochs-1):\n",
    "        dy = pi_grads[n+1] - pi_grads[n]\n",
    "        dx = steps[n+1] - steps[n]\n",
    "        pi_grad_grads.append( (dy / dx) )\n",
    "\n",
    "    if debug_show: \n",
    "        print(pi_grads)\n",
    "        print(pi_grad_grads)\n",
    "    \n",
    "\n",
    "    ## maximum pi grad\n",
    "    max_grad_ind = max(list(range(epochs-1)), key= lambda ind : pi_grads[ind])\n",
    "    total_ind = max_grad_ind\n",
    "        \n",
    "\n",
    "\n",
    "    total_ind = min(max(total_ind, 0), epochs -1)\n",
    "\n",
    "    return total_ind\n",
    "\n",
    "def create_file_paths(path, seeds, filename, FILES):\n",
    "    for seed in seeds:\n",
    "        FILES.append(path + filename + str(seed)+'.csv')\n",
    "    return FILES\n",
    "    \n",
    "def generate_reward_list(exp_name):\n",
    "    exp1 = ExperimentVisualise(id=exp_name, version=1.1, plot_version=1, test_seeds=5000)\n",
    "\n",
    "    disp_arr = display_commands_v2(command)\n",
    "    \n",
    "    exp_vis = {\n",
    "                '1':{ \"id\":exp1, \"color\":'r',\"show\": disp_arr[1], \"label\":exp_name},\n",
    "    }\n",
    "\n",
    "    metric = 'reward'\n",
    "\n",
    "    path, seeds, filename = exp_vis['1']['id'].get_file_paths()\n",
    "    FILES = []\n",
    "    FILES = create_file_paths(path, seeds, filename, FILES)\n",
    "    cur_length, full_arr, refined = [], [], []\n",
    "    for file in FILES:\n",
    "        reward_summary = pd.read_csv(file)\n",
    "        cur_length.append(reward_summary.shape[0])\n",
    "        full_arr.append(reward_summary[metric])\n",
    "    for x in full_arr:\n",
    "        refined.append(x[0:min(cur_length)])\n",
    "    data = pd.concat(refined, axis=1)\n",
    "    data['mean'] = data.mean(axis=1)\n",
    "    \n",
    "    return list(data['mean'])\n",
    "    \n",
    "def generate_end_index(exp_name, return_proportion=False, debug_show=False,use_alt=False):\n",
    "    df = pd.read_csv(MAIN_PATH + '/' + 'results/' + exp_name + '/experiment_summary.csv', header=\"infer\")\n",
    "    df_len = len(df[\"value_grad\"])\n",
    "    \n",
    "    exp = ExperimentVisualise(id=exp_name, version=1.1, plot_version=1, test_seeds=5000)\n",
    "    d1 = exp.get_training_logs()\n",
    "\n",
    "    steps = list(d1['steps'])\n",
    "    val_grad = list(d1['value_grad'])\n",
    "    val_loss = list(d1['val_loss'])\n",
    "    pi_grad = list(d1['policy_grad'])\n",
    "\n",
    "    if use_alt: ind = check_end_condition_total_alt(val_grad, val_loss, pi_grad, steps, debug_show=debug_show)\n",
    "    else: ind = check_end_condition_total(val_grad, val_loss, pi_grad, steps, debug_show=debug_show)\n",
    "    if return_proportion: return ind / (df_len - 1)\n",
    "    return ind\n",
    "\n",
    "def take_rows(exp_name, offset_ind=5000, workers=5, episode=0):\n",
    "    exp_folder = MAIN_PATH + '/results/' + exp_name + '/' \n",
    "\n",
    "    dfs = []\n",
    "    files = [exp_folder + f'testing/worker_episode_summary_{offset_ind + n}.csv' for n in range(workers)]\n",
    "    for file in files:\n",
    "        df = pd.read_csv(file)\n",
    "        df[\"full_length\"] = (df['t'] == 288).astype(int)\n",
    "        df[\"adj_normo\"] = (df[\"t\"] / 288) * df[\"normo\"]\n",
    "        # print(df[\"adj_normo\"][:5])\n",
    "        # print(df[\"t\"][:5])\n",
    "        # print(df[\"normo\"][:5])\n",
    "        if episode != None:\n",
    "            df_filt = df[df['epi'] == episode]\n",
    "            dfs.append(df_filt)\n",
    "        else:\n",
    "            dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "def get_mn_sd(li):\n",
    "    return (np.mean(li), np.std(li))\n",
    "\n",
    "def find_best_tir_index(exp_name, bound, workers=5): #bound exclusive\n",
    "    ep_dfs = []\n",
    "    for epi in range(bound):\n",
    "        ep_dfs.append(take_rows(exp_name, episode=epi+1, workers=workers))\n",
    "\n",
    "    return max(list(range(bound)), key = lambda ind: get_mn_sd(ep_dfs[ind][\"adj_normo\"])[0])\n",
    "\n",
    "def find_best_reward_index(exp_name, bound, workers=5): #bound exclusive\n",
    "    ep_dfs = []\n",
    "    for epi in range(bound):\n",
    "        ep_dfs.append(take_rows(exp_name, episode=epi+1, workers=workers))\n",
    "\n",
    "    return max(list(range(bound)), key = lambda ind: get_mn_sd(ep_dfs[ind][\"reward\"])[0])\n",
    "\n",
    "# modified graphing functions\n",
    "\n",
    "def plot_testing_average_metric(dict, groups, type, dis_len, metric, goal, fill,vline=None, title=None):\n",
    "    fig = plt.figure(figsize=(16,9))\n",
    "    ax = fig.add_subplot(111)\n",
    "\n",
    "    for groupings in range(0, len(groups)):\n",
    "        FILES = []\n",
    "        for i in groups[groupings]:  # exp's inside the group\n",
    "            # will give the exp list\n",
    "            path, seeds, filename = dict[i]['id'].get_file_paths()\n",
    "            FILES = create_file_paths(path, seeds, filename, FILES)\n",
    "        cur_length, full_arr, refined = [], [], []\n",
    "        for file in FILES:\n",
    "            reward_summary = pd.read_csv(file)\n",
    "            cur_length.append(reward_summary.shape[0])\n",
    "            full_arr.append(reward_summary[metric])\n",
    "        for x in full_arr:\n",
    "            refined.append(x[0:min(cur_length)])\n",
    "        data = pd.concat(refined, axis=1)\n",
    "        data['mean'] = data.mean(axis=1)\n",
    "\n",
    "        if type == 'normal':\n",
    "            data['std_dev'] = data.std(axis=1)\n",
    "            data['max'] = data['mean'] + data['std_dev']  # * 2\n",
    "            data['min'] = data['mean'] - data['std_dev']  # * 2\n",
    "        else:\n",
    "            data['max'] = data.max(axis=1)\n",
    "            data['min'] = data.min(axis=1)\n",
    "\n",
    "        data['steps'] = np.arange(len(data))\n",
    "        data['steps'] = (data['steps'] + 1) * dict[i]['id'].training_workers * dict[i]['id'].args['agent']['n_step']\n",
    "\n",
    "        ax.plot(data['steps'], data['mean'], color=dict[i]['color'], label=dict[i]['label'])\n",
    "        if fill:\n",
    "            ax.fill_between(data['steps'], data['min'], data['max'], color=dict[i]['color'], alpha=0.1)\n",
    "\n",
    "    ax.axhline(y=goal, color='k', linestyle='--')\n",
    "    \n",
    "    start_step = list(data['steps'])[0]\n",
    "    end_step = list(data['steps'])[-1]\n",
    "    if vline != None:\n",
    "        ax.axvline(x=start_step + vline*(end_step - start_step), color='b', linestyle='-')\n",
    "\n",
    "    graph_title =  title if title is not None else 'Training Curve'\n",
    "    # ax.set_title(graph_title, fontsize=32)\n",
    "    # ax.legend(loc=\"upper left\", fontsize=16)\n",
    "    ax.set_ylabel('Average Validation Reward', fontsize=TITLE_FONT_SIZE) #ax.set_ylabel(metric)\n",
    "    ax.set_xlabel('Training Interactions', fontsize=TITLE_FONT_SIZE)\n",
    "    ax.grid()\n",
    "    ax.set_xticks([])\n",
    "    ax.set_xlim(0, dis_len)\n",
    "    ax.set_ylim(0, 320)\n",
    "    ax.tick_params(axis='y', which='major', labelsize=AXIS_FONT_SIZE) \n",
    "    plt.show()\n",
    "\n",
    "def plot_training_logs(mode, exp_dict, dis_len, params,vline=None, cols=1,val_grad_poly=None):\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    subplots = []\n",
    "    tot_plots = len(params)\n",
    "\n",
    "    for i in range(0, tot_plots):\n",
    "        subplots.append(fig.add_subplot(math.ceil(tot_plots/cols), cols, i+1))\n",
    "        subplots[i].grid(True)\n",
    "        subplots[i].set_xlim(0, dis_len)\n",
    "        subplots[i].set_title( \"\" )#params[i])\n",
    "        subplots[i].set_xlabel( \"Interactions\", fontsize=TITLE_FONT_SIZE )#params[i])\n",
    "        subplots[i].set_ylabel( \"Summed Policy Gradient\", fontsize=TITLE_FONT_SIZE )#params[i])\n",
    "        subplots[i].set_xticks([])\n",
    "        subplots[i].tick_params(axis='y', which='major', labelsize=AXIS_FONT_SIZE) \n",
    "\n",
    "    for exp in exp_dict:\n",
    "        if exp_dict[exp]['show']:\n",
    "            if mode == 'ppo':\n",
    "                # ['exp_var', 'true_var','val_loss', 'policy_grad', 'value_grad', 'pi_loss', 'avg_rew']\n",
    "                d1 = exp_dict[exp]['id'].get_training_logs()\n",
    "            elif mode == 'aux':\n",
    "                # ['pi_aux_loss', 'vf_aux_loss', 'pi_aux_grad', 'vf_aux_grad']\n",
    "                d1 = exp_dict[exp]['id'].get_aux_training_logs()\n",
    "            elif mode == 'planning':\n",
    "                # ['plan_grad', 'plan_loss']\n",
    "                d1 = exp_dict[exp]['id'].get_planning_training_logs()\n",
    "            else:\n",
    "                d1 = 0\n",
    "                print('Invalid mode selected')\n",
    "                exit()\n",
    "            for j in range(0, tot_plots):\n",
    "                start_step = list(d1['steps'])[0]\n",
    "                end_step = list(d1['steps'])[-1]\n",
    "\n",
    "                if params[j] == 'value_grad' and val_grad_poly != None:\n",
    "                    polyline = np.linspace(start_step, end_step)\n",
    "                    subplots[j].plot(polyline, val_grad_poly(polyline))\n",
    "                subplots[j].plot(d1['steps'], d1[params[j]], color=exp_dict[exp]['color'], label=exp_dict[exp]['id'].id)\n",
    "                subplots[j].axvline(x=start_step + vline*(end_step - start_step), color='b', linestyle='-')\n",
    "\n",
    "    # for i in range(0, tot_plots):\n",
    "    #     subplots[i].legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "def calculate_r_squared(y_data, RSS):\n",
    "    y_mean = np.mean(y_data)\n",
    "    TSS = sum([(y_mean - y)**2 for y in y_data])\n",
    "    return 1 - (RSS/TSS)\n",
    "\n",
    "def generate_poly(steps, val_grad):\n",
    "    fitted_poly = poly.Polynomial.fit(steps, val_grad, 2, full=True) #fit quadratic model\n",
    "    r_2 = calculate_r_squared(val_grad, fitted_poly[1][0][0])\n",
    "    return r_2, fitted_poly[0]\n",
    "\n",
    "# main\n",
    "\n",
    "command = []\n",
    "# dis_len = 1.7 * 100000 #BC\n",
    "# dis_len = 2.5 * 1000000 #TD3+BC\n",
    "dis_len = 3.5 * 100000 #IQL\n",
    "\n",
    "disp_arr = display_commands_v2(command)\n",
    "\n",
    "exp1 = ExperimentVisualise(id=EXP_NAME, version=1.1, plot_version=1, test_seeds=5000)\n",
    "\n",
    "exp_vis = { '1':{ \"id\":exp1, \"color\":'r',\"show\": disp_arr[1], \"label\":LABEL}}\n",
    "\n",
    "exp_logs = exp_vis['1']['id'].get_training_logs()\n",
    "goodness_val_grad_fit, val_grad_poly = generate_poly(list(exp_logs['steps']), list(exp_logs['value_grad']))\n",
    "\n",
    "df = pd.read_csv(MAIN_PATH + '/' + 'results/' + EXP_NAME + '/experiment_summary.csv', header=\"infer\")\n",
    "df_len = len(df[\"value_grad\"])\n",
    "\n",
    "if TAKE_BEST:\n",
    "    chosen_ind = find_best_reward_index(EXP_NAME,df_len,workers=4)\n",
    "    chosen_ind_prop = (chosen_ind) / (df_len - 1)\n",
    "else:\n",
    "    chosen_ind_prop = generate_end_index(EXP_NAME, True, debug_show=True, use_alt=USE_IQL_CRITERIA)\n",
    "    chosen_ind = generate_end_index(EXP_NAME, use_alt=USE_IQL_CRITERIA)\n",
    "    print(\"\\tTerminated on index\",chosen_ind)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "exps = []\n",
    "for patient_id in list(range(0,10)) + list(range(20,30)):\n",
    "    for seed in range(3):\n",
    "        exps.append( f\"{NAME_PREFIX}_{patient_id}_{seed}\" )\n",
    "\n",
    "percents = []\n",
    "for exp_name in exps:\n",
    "    \n",
    "    print(f\"{exp_name}: \")\n",
    "    reward_list = generate_reward_list(exp_name)\n",
    "    chosen_ind = generate_end_index(exp_name,debug_show=False, use_alt=False)\n",
    "    opt_ind = find_best_tir_index(exp_name, bound=len(reward_list), workers=4)\n",
    "\n",
    "    percent = 100 * reward_list[chosen_ind] / max(reward_list)\n",
    "\n",
    "    print(f\"\\tReward obtained is {reward_list[chosen_ind]:.2f}/{max(reward_list):.2f}, {percent:.2f}%\")\n",
    "    print()\n",
    "\n",
    "    percents.append(percent)\n",
    "\n",
    "\n",
    "print(f\"Average returns of {np.mean(percents):.2f}+-{np.std(percents):.2f}%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e2acc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== offline_td3_purebc_0_0: \n",
      "\tReward obtained is 283.82/283.82, 100.00%, on index 7/16\n",
      "patient_id 0\n",
      "['adolescent#001', 'adolescent#002', 'adolescent#003', 'adolescent#004', 'adolescent#005', 'adolescent#006', 'adolescent#007', 'adolescent#008', 'adolescent#009', 'adolescent#010', 'child#001', 'child#002', 'child#003', 'child#004', 'child#005', 'child#006', 'child#007', 'child#008', 'child#009', 'child#010', 'adult#001', 'adult#002', 'adult#003', 'adult#004', 'adult#005', 'adult#006', 'adult#007', 'adult#008', 'adult#009', 'adult#010']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Crago\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adolescent#001', 'adolescent#002', 'adolescent#003', 'adolescent#004', 'adolescent#005', 'adolescent#006', 'adolescent#007', 'adolescent#008', 'adolescent#009', 'adolescent#010', 'child#001', 'child#002', 'child#003', 'child#004', 'child#005', 'child#006', 'child#007', 'child#008', 'child#009', 'child#010', 'adult#001', 'adult#002', 'adult#003', 'adult#004', 'adult#005', 'adult#006', 'adult#007', 'adult#008', 'adult#009', 'adult#010']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Crago\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adolescent#001', 'adolescent#002', 'adolescent#003', 'adolescent#004', 'adolescent#005', 'adolescent#006', 'adolescent#007', 'adolescent#008', 'adolescent#009', 'adolescent#010', 'child#001', 'child#002', 'child#003', 'child#004', 'child#005', 'child#006', 'child#007', 'child#008', 'child#009', 'child#010', 'adult#001', 'adult#002', 'adult#003', 'adult#004', 'adult#005', 'adult#006', 'adult#007', 'adult#008', 'adult#009', 'adult#010']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Crago\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adolescent#001', 'adolescent#002', 'adolescent#003', 'adolescent#004', 'adolescent#005', 'adolescent#006', 'adolescent#007', 'adolescent#008', 'adolescent#009', 'adolescent#010', 'child#001', 'child#002', 'child#003', 'child#004', 'child#005', 'child#006', 'child#007', 'child#008', 'child#009', 'child#010', 'adult#001', 'adult#002', 'adult#003', 'adult#004', 'adult#005', 'adult#006', 'adult#007', 'adult#008', 'adult#009', 'adult#010']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Crago\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\envs\\registration.py:17: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 161\u001b[0m\n\u001b[0;32m    159\u001b[0m use_iql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miql\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m exp_name\n\u001b[0;32m    160\u001b[0m delete_files_with_prefix(exp_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting/\u001b[39m\u001b[38;5;124m'\u001b[39m, prefix1\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_episode_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(VLD_OFFSET)[\u001b[38;5;241m0\u001b[39m], prefix2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mworker_episode_summary_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(VLD_OFFSET)[\u001b[38;5;241m0\u001b[39m], show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 161\u001b[0m \u001b[43mrun_validation_trials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchosen_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_iql\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_iql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew validation trials ran.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[7], line 107\u001b[0m, in \u001b[0;36mrun_validation_trials\u001b[1;34m(exp_name, checkpoint, use_iql)\u001b[0m\n\u001b[0;32m    104\u001b[0m     policy \u001b[38;5;241m=\u001b[39m TD3BC_ActorCritic(args, load\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, actor_path\u001b[38;5;241m=\u001b[39mactor_path, critic_path\u001b[38;5;241m=\u001b[39mcritic_path, device\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;66;03m#TD3+BC ONLY\u001b[39;00m\n\u001b[0;32m    106\u001b[0m logger \u001b[38;5;241m=\u001b[39m Logger(args)\n\u001b[1;32m--> 107\u001b[0m validation_agents \u001b[38;5;241m=\u001b[39m [\u001b[43mOnPolicyWorker\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtesting\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_agent_id_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mn_val_trials)]\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mn_val_trials):\n",
      "File \u001b[1;32m~\\Desktop\\Uni\\Y4S1\\hons\\Coding\\RL4T1D\\utils\\worker.py:28\u001b[0m, in \u001b[0;36mOnPolicyWorker.__init__\u001b[1;34m(self, args, env_args, mode, worker_id)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, env_args, mode, worker_id):\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mWorker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Uni\\Y4S1\\hons\\Coding\\RL4T1D\\utils\\worker.py:11\u001b[0m, in \u001b[0;36mWorker.__init__\u001b[1;34m(self, args, env_args, mode, worker_id)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, env_args, mode, worker_id):\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mT1DEnv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_args \u001b[38;5;241m=\u001b[39m env_args\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworker_id \u001b[38;5;241m=\u001b[39m worker_id\n",
      "File \u001b[1;32m~\\Desktop\\Uni\\Y4S1\\hons\\Coding\\RL4T1D\\environment\\t1denv.py:13\u001b[0m, in \u001b[0;36mT1DEnv.__init__\u001b[1;34m(self, args, mode, worker_id)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m get_env(args, worker_id\u001b[38;5;241m=\u001b[39mworker_id, env_type\u001b[38;5;241m=\u001b[39mmode)  \u001b[38;5;66;03m# setup environment\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Uni\\Y4S1\\hons\\Coding\\RL4T1D\\environment\\t1denv.py:16\u001b[0m, in \u001b[0;36mT1DEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n",
      "File \u001b[1;32mc:\\Users\\Crago\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\gym\\core.py:104\u001b[0m, in \u001b[0;36mEnv.reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     99\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Resets the state of the environment and returns an initial observation.\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03m    Returns: observation (object): the initial observation of the\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m        space.\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\Uni\\Y4S1\\hons\\Coding\\RL4T1D\\environment\\extended_T1DSimEnv.py:108\u001b[0m, in \u001b[0;36mT1DSimEnv._reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m obs, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_space\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcgm\u001b[39m\u001b[38;5;124m'\u001b[39m: obs\u001b[38;5;241m.\u001b[39mCGM, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsulin\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_to_meal\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    107\u001b[0m                                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuture_carb\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeal_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_hour\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_min\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m--> 108\u001b[0m cur_cgm, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalibration_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_state, info\n",
      "File \u001b[1;32m~\\Desktop\\Uni\\Y4S1\\hons\\Coding\\RL4T1D\\environment\\extended_T1DSimEnv.py:135\u001b[0m, in \u001b[0;36mT1DSimEnv.calibration_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinit_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinit_flag:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cur_cgm, info\n",
      "File \u001b[1;32m~\\Desktop\\Uni\\Y4S1\\hons\\Coding\\RL4T1D\\environment\\extended_T1DSimEnv.py:108\u001b[0m, in \u001b[0;36mT1DSimEnv._reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m obs, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_space\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcgm\u001b[39m\u001b[38;5;124m'\u001b[39m: obs\u001b[38;5;241m.\u001b[39mCGM, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsulin\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_to_meal\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    107\u001b[0m                                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuture_carb\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeal_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_hour\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_min\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m--> 108\u001b[0m cur_cgm, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalibration_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_state, info\n",
      "File \u001b[1;32m~\\Desktop\\Uni\\Y4S1\\hons\\Coding\\RL4T1D\\environment\\extended_T1DSimEnv.py:135\u001b[0m, in \u001b[0;36mT1DSimEnv.calibration_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinit_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinit_flag:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cur_cgm, info\n",
      "    \u001b[1;31m[... skipping similar frames: T1DSimEnv._reset at line 108 (4 times), T1DSimEnv.calibration_process at line 135 (3 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\Desktop\\Uni\\Y4S1\\hons\\Coding\\RL4T1D\\environment\\extended_T1DSimEnv.py:135\u001b[0m, in \u001b[0;36mT1DSimEnv.calibration_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinit_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreinit_flag:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cur_cgm, info\n",
      "File \u001b[1;32m~\\Desktop\\Uni\\Y4S1\\hons\\Coding\\RL4T1D\\environment\\extended_T1DSimEnv.py:108\u001b[0m, in \u001b[0;36mT1DSimEnv._reset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    105\u001b[0m obs, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_space\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcgm\u001b[39m\u001b[38;5;124m'\u001b[39m: obs\u001b[38;5;241m.\u001b[39mCGM, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minsulin\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime_to_meal\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m    107\u001b[0m                                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuture_carb\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeal_type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_hour\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday_min\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m--> 108\u001b[0m cur_cgm, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalibration_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_state, info\n",
      "File \u001b[1;32m~\\Desktop\\Uni\\Y4S1\\hons\\Coding\\RL4T1D\\environment\\extended_T1DSimEnv.py:118\u001b[0m, in \u001b[0;36mT1DSimEnv.calibration_process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m future_carb, remaining_time, day_hour, day_min, meal_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannounce_meal(meal_announce\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    117\u001b[0m state, reward, is_done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(act)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcur_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcgm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCGM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minsulin\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd_basal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtime_to_meal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfuture_carb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture_carb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmeal_type\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeal_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mday_hour\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mday_hour\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mday_min\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mday_min\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m cur_cgm \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mCGM\n\u001b[0;32m    123\u001b[0m info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m state\n",
      "File \u001b[1;32m~\\Desktop\\Uni\\Y4S1\\hons\\Coding\\RL4T1D\\environment\\obs_space.py:18\u001b[0m, in \u001b[0;36mObservationSpace.update\u001b[1;34m(self, dict)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_dict[key]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mdict\u001b[39m[key])\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" \n",
    "5000 - Validation during training\n",
    "8000 - early stopping\n",
    "9000 - picking best adj normoglycemia (TIR)\n",
    "10000 - picking best reward\n",
    "\"\"\"\n",
    "\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, MAIN_PATH + '/experiments/')\n",
    "\n",
    "TRIAL_N = 50\n",
    "VLD_OFFSET = 9000\n",
    "IQL_EARLY_STOPPING = True\n",
    "\n",
    "def open_arg_file(file_dest):\n",
    "    with open(file_dest,'r') as fp:\n",
    "        args_dict = json.load(fp)\n",
    "        fp.close()\n",
    "    return OmegaConf.create(args_dict)\n",
    "\n",
    "def rec_alter(tdict, replacee, replacer):\n",
    "    for k in tdict:\n",
    "        if type(tdict[k]) == type(dict()):\n",
    "            tdict[k] = rec_alter(tdict[k], replacee, replacer)\n",
    "        else:\n",
    "            if type(tdict[k]) == type(\"\"): \n",
    "                tdict[k] = tdict[k].replace(replacee, replacer)\n",
    "    return tdict\n",
    "\n",
    "def zombify_arg_file(file_dest):\n",
    "    with open(file_dest,'r') as fp:\n",
    "        args_dict = json.load(fp)\n",
    "        fp.close()\n",
    "    \n",
    "    nest_keys = []\n",
    "    for k in args_dict.keys():\n",
    "        if type(args_dict[k]) == type(dict()):\n",
    "            nest_keys.append(k)\n",
    "    \n",
    "    args_dict[\"agent\"][\"validation_agent_id_offset\"] = VLD_OFFSET\n",
    "    args_dict[\"agent\"][\"n_val_trials\"] = TRIAL_N\n",
    "    args_dict[\"agent\"][\"n_testing_workers\"] = 0\n",
    "    args_dict[\"agent\"][\"n_training_workers\"] = 0\n",
    "    args_dict[\"logger\"][\"experiment_logs\"] = []\n",
    "\n",
    "    for k in nest_keys:\n",
    "        for n_k in args_dict[k].keys():\n",
    "            if not n_k in args_dict: args_dict[n_k] = (args_dict[k])[n_k]\n",
    "    \n",
    "    rec_alter(args_dict, f\"../results/\", MAIN_PATH + '/results/')\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    return OmegaConf.create(args_dict)\n",
    "\n",
    "def delete_files_with_prefix(folder_path, prefix1=\"worker_episode_8\", prefix2=\"worker_episode_summary_8\", show=False) -> None:\n",
    "    \"\"\"\n",
    "    Delete all files in the given folder whose names start with the given prefix.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing files.\n",
    "        prefix (str): The string prefix to match filenames against.\n",
    "    \"\"\"\n",
    "    folder = Path(folder_path)\n",
    "\n",
    "    if not folder.is_dir():\n",
    "        raise ValueError(f\"{folder_path} is not a valid directory.\")\n",
    "\n",
    "    for file in folder.iterdir():\n",
    "        if file.is_file() and (file.name.startswith(prefix1) or file.name.startswith(prefix2)):\n",
    "            try:\n",
    "                file.unlink()  # Deletes the file\n",
    "                if show: print(f\"Deleted: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file}: {e}\")\n",
    "\n",
    "def run_validation_trials(exp_name, checkpoint, use_iql=False):\n",
    "    exp_folder = MAIN_PATH + '/results/' + exp_name + '/' \n",
    "\n",
    "    \n",
    "    # args = open_arg_file(exp_folder + 'args.json')\n",
    "    args = zombify_arg_file(exp_folder + 'args.json')\n",
    "    args.n_action = args.n_actions\n",
    "\n",
    "    print(\"patient_id\", args.patient_id)\n",
    "\n",
    "    \n",
    "    if use_iql: #IQL \n",
    "        actor_path = exp_folder + f'checkpoints/episode_{max(0,checkpoint)}_policy_net.pth'\n",
    "        critic_path = exp_folder + f'checkpoints/episode_{max(0,checkpoint)}_critic_net.pth'\n",
    "        value_path = exp_folder + f'checkpoints/none.pth'\n",
    "\n",
    "        # actor_path = exp_folder + f'checkpoints/episode_{max(0,checkpoint)}_policy_net.pth'\n",
    "        # critic_path = exp_folder + f'checkpoints/episode_{max(0,checkpoint)}_critic_net.pth'\n",
    "        # value_path = exp_folder + f'checkpoints/episode_{max(0,checkpoint)}_value_net.pth'\n",
    "\n",
    "        policy = IQL_ActorCritic(args, load=True, actor_path=actor_path, critic_path=critic_path, value_path=value_path, device=args.agent.device).to(args.device) #TD3+BC ONLY\n",
    "    else: #TD3+BC \n",
    "        actor_path = exp_folder + f'checkpoints/episode_{max(0,checkpoint)}_policy_net.pth'\n",
    "        critic_path = exp_folder + f'checkpoints/episode_{max(0,checkpoint)}_value_net.pth'\n",
    "        policy = TD3BC_ActorCritic(args, load=True, actor_path=actor_path, critic_path=critic_path, device=args.agent.device).to(args.device) #TD3+BC ONLY\n",
    "\n",
    "    logger = Logger(args)\n",
    "    validation_agents = [OnPolicyWorker(args=args, env_args=args, mode='testing', worker_id=i + args.validation_agent_id_offset) for i in range(args.n_val_trials)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(args.agent.n_val_trials):\n",
    "            validation_agents[i].rollout(policy=policy, buffer=None, logger=logger.logWorker)\n",
    "    \n",
    "     # calculate the final metrics.\n",
    "    cohort_res, summary_stats = [], []\n",
    "    secondary_columns = ['epi', 't', 'reward', 'normo', 'hypo', 'sev_hypo', 'hyper', 'lgbi',\n",
    "                    'hgbi', 'ri', 'sev_hyper', 'aBGP_rmse', 'cBGP_rmse']\n",
    "    data = []\n",
    "    FOLDER_PATH = args.experiment_folder+'/testing/'\n",
    "    for i in range(0, args.n_val_trials):\n",
    "        test_i = 'worker_episode_'+str(args.validation_agent_id_offset+i)+'.csv'\n",
    "        df = pd.read_csv(FOLDER_PATH+ '/'+test_i, dtype=float)\n",
    "        normo, hypo, sev_hypo, hyper, lgbi, hgbi, ri, sev_hyper = time_in_range(df['cgm'])\n",
    "        reward_val = df['rew'].sum()*(100/288)\n",
    "        e = [[i, df.shape[0], reward_val, normo, hypo, sev_hypo, hyper, lgbi, hgbi, ri, sev_hyper, 0, 0]]\n",
    "        dataframe = pd.DataFrame(e, columns=secondary_columns)\n",
    "        data.append(dataframe)\n",
    "    res = pd.concat(data)\n",
    "    res['PatientID'] = args.patient_id\n",
    "    res.rename(columns={'sev_hypo':'S_hypo', 'sev_hyper':'S_hyper'}, inplace=True)\n",
    "    summary_stats.append(res)\n",
    "    metric=['mean', 'std', 'min', 'max']\n",
    "    print(calc_stats(res, metric=metric, sim_len=288))\n",
    "\n",
    "    print('\\nAlgorithm Training/Validation Completed Successfully.')\n",
    "    print('---------------------------------------------------------')\n",
    "    exit()\n",
    "\n",
    "exp_names = exps\n",
    "\n",
    "for exp_name in exp_names:\n",
    "    exp_folder = MAIN_PATH + '/results/' + exp_name + '/' \n",
    "    patient_id = int(exp_name.split('_')[-2])\n",
    "    seed = int(exp_name.split('_')[-1])\n",
    "\n",
    "    print(f\"========== {exp_name}: \")\n",
    "    reward_list = generate_reward_list(exp_name)\n",
    "\n",
    "    # chosen_ind = generate_end_index(exp_name,debug_show=True,use_alt=IQL_EARLY_STOPPING)\n",
    "    chosen_ind = find_best_reward_index(exp_name, bound=len(reward_list), workers=4) \n",
    "    # chosen_ind_alt = find_best_tir_index(exp_name, bound=len(reward_list), workers=4) \n",
    "\n",
    "    # print(exp_name, chosen_ind, chosen_ind_alt, chosen_ind_alt==chosen_ind)\n",
    "    percent = 100 * reward_list[chosen_ind] / max(reward_list)\n",
    "\n",
    "    print(f\"\\tReward obtained is {reward_list[chosen_ind]:.2f}/{max(reward_list):.2f}, {percent:.2f}%, on index {chosen_ind}/{len(reward_list)}\")\n",
    "\n",
    "    # early_stopping_policy_dest = exp_folder + f'checkpoints/episode_{chosen_ind}_policy_net.pth'\n",
    "\n",
    "    use_iql = 'iql' in exp_name\n",
    "    delete_files_with_prefix(exp_folder + 'testing/', prefix1=\"worker_episode_\" + str(VLD_OFFSET)[0], prefix2=\"worker_episode_summary_\" + str(VLD_OFFSET)[0], show=False)\n",
    "    run_validation_trials(exp_name, chosen_ind, use_iql=use_iql)\n",
    "    print(\"New validation trials ran.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
