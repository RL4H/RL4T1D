agent: 'td3_bc'
debug: False

n_rnn_hidden: 16
n_rnn_layers: 1
rnn_directions: 1
bidirectional: False

# parameters for RL discounting strategies
return_type: 'average'   # discount | average; is average gamma and lamda -> 1.
gamma: 0.9
lambda_: 0.95

# parameters for the RL rollout
normalize_reward: True  # reward received is normalised
shuffle_rollout: True  # rollout trajectory data is shuffled
replay_buffer_size: 65536
replay_buffer_step: 8192
preserve_trajectories : False

# ppo parameters
entropy_coef: 0.001
grad_clip: 20
eps_clip: 0.1
target_kl: 0.01

# train
n_step: 8192
max_epi_length: 2880
n_training_workers: 16
total_interactions: 4000000 # total number of interactions the agent will train for #800000
n_interactions_lr_decay: 4000000

# training parameters
pi_lr: 0.001
vf_lr: 0.003

batch_size : 8192
mini_batch_size: 8192
n_pi_epochs: 2
target_update_interval: 1

# test
n_testing_workers: 4
max_test_epi_len: 288  # testing conducted for 1 day: 12 steps/hr * 24 hours

# validation
n_val_trials: 10

#data source
data_preload: true,
data_type: "simulated" #simulated | clinical
data_protocols: [] #None defaults to all
data_algorithms : [] #None defaults to all
use_all_interactions : false

use_cirriculum : false
cirriculum_horizon : 6400000 #DISABLED
post_horizon_data_protocols: ["evaluation"]
post_horizon_data_algorithms: ["G2P2C","AUXML", "PPO","TD3"]

vf_pretrain_iters : 0
bc_pretrain_iters : 0 #160000

n_handcrafted_features: -10 #FIXME
n_features: 32
use_handcraft: 0

n_hidden: 16
noise_model: "normal_dist"
# noise_std: 0.02
# noise_application: 3
# target_action_std: 0.03
# target_action_lim: 0.1
# soft_tau: 0.005

noise_std: 0.05
noise_application: 3
target_action_std: 0.5
target_action_lim: 0.5
soft_tau: 0.007

push_action_clip: false


alpha : 1
beta : 1

awc_enabled : false
beta_awac : 1

vf_lambda : 0
pi_lambda : 0

pi_clip : 10
